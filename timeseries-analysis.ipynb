{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faef425c-f9b8-4609-9861-af4a3ed0e1af",
   "metadata": {},
   "source": [
    "## Zeiss Coding Challenge: Data Scientist\n",
    "**Handling gaps in time series dataset from temperature readings for the development of anomaly detection algorithm.**\n",
    "\n",
    "Author: Maximilian Kapsecker <br>\n",
    "Contact: max.kapsecker@gmail.com <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d46c32c-7f7e-4854-a13d-e5381440eb4a",
   "metadata": {},
   "source": [
    "### 1. Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a3d853-76fc-4ef5-9b5b-0a39b6492797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from astropy.timeseries import LombScargle\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from src.helper import Helper\n",
    "from src.window_generator import WindowGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3eae8-8274-49ae-9a30-fcd62a04d667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fostering reproducibility for \"random\" computations\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f153c-0de9-4b8e-9906-bea5e11b4394",
   "metadata": {},
   "source": [
    "### 2. Data Loading and Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de38bb-999f-42a1-8f21-8dbe328f865c",
   "metadata": {},
   "source": [
    "The data source could be a cloud bucket, a database, or any interface for streaming data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bdcd84-488e-460b-b406-ef2a88c2f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/sample_temperature_data_for_coding_challenge.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b810f-4700-48e3-a441-300b3ca13504",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3fba0-9c03-4fce-8f34-c1a3ff0d9daa",
   "metadata": {},
   "source": [
    "Additionally, depending on the context, it might be beneficial to consider external data, such as general weather conditions, e.g, if the device or measurement is exposed to sunlight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c034016-7e32-4805-a4bc-56b0b3e3bef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Helper.print_unique_column_entries(df, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a1c25e-6114-483c-9cc1-71fa9c4b0822",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"property_name\").describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aab7919-377e-46fb-84be-9586b34eb1ef",
   "metadata": {},
   "source": [
    "<br>From the overview we assume:\n",
    "- The data comes from a single source, potentially a microcontroller\n",
    "- The central measured value is the temperature -- probably in degrees Celsius\n",
    "- The temperature values are divided into either heating or cooling.\n",
    "- Some heating and cooling information is available for exactly the same timestamp (e.g., row ```0``` and ```1```), which means that it is not a real-time measurement of the current system temperature, but a kind of calculated property that determines the setpoints for activating cooling or heating mode.\n",
    "\n",
    "<b>Assumption 1:</b> The data comes from a single microcontroller that specifies the target temperatures [C°] at which either cooling or heating is to take place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39986bc2-56ae-444a-8ca4-ce33b4fe28a3",
   "metadata": {},
   "source": [
    "### 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f3f25-2871-48c1-92ad-80da391c7f0c",
   "metadata": {},
   "source": [
    "For reasons of simplicity and visualization, the ```property_names``` entries are renamed. Further, to avoid errors when accessing the ```property_name```, e.g., filtering , we save the two category names as constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd55b0b7-1e3e-41f4-a8a6-3f47a8cf221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROP_HEATING = \"Heating\"\n",
    "PROP_COOLING = \"Cooling\"\n",
    "df.property_name = df.property_name.replace(\"heating_temperature\", PROP_HEATING).replace(\"cooling_temperature\", PROP_COOLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5b434b-bc27-4974-a429-c6cffebd8998",
   "metadata": {},
   "source": [
    "Based on the insights from the overview, we infer the datatypes and cast the column entries accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c317983e-7edf-4c27-b332-a8b92e27de27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"source_id\": 'category', \"property_name\": 'category', \"temperature\": 'float64'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f21005-f549-4f8c-ac6f-a977715de0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No errors with the following command -> all entries represent a valid datetime \n",
    "df.datetime = pd.to_datetime(df.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a967d9-2f68-4c9e-8a7a-9cb312b190ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Do we have any NaN inside the dataframe:\", df.isnull().values.any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a33a16-be45-488e-ad8b-db16832404db",
   "metadata": {},
   "source": [
    "For the purpose of this analysis, some operations might yield wrong results if the rows are not sorted by datetime, e.g., taking the time difference of two subsequent rows. Therefore, we sort the dateframe by the datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e520ae-c0cc-41d6-a080-9e95b525bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"datetime\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060843a6-dd57-455d-8c83-8476e31ee700",
   "metadata": {},
   "source": [
    "The column ```source_id``` can be removed from the following analysis because it contains only a single unique entry. Dropping it offers advantages in terms of computing resources, specifically when the size of the table grows fast. However, please note that if a more generalized solution is needed, <b>keeping the column might be necessary</b>, as there could be multiple sources requiring different handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3532b15-1598-4a08-91da-5489434f1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"source_id\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f08df6-80c3-4959-bc2e-fc4068472b07",
   "metadata": {},
   "source": [
    "### 4. Explorative Data Analysis\n",
    "The objective of the exploratory data analysis (EDA) is twofold:\n",
    "- To gain insights into the data through visualizations, simple statistics, and pattern analysis. The aim here is to ask relevant questions that contribute to a better understanding of the data.\n",
    "- To refine the task formulation based on the overall title: _Handling gaps in a time series dataset from temperature readings for the development of an anomaly detection algorithm._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22703703-abdf-4725-88a4-4f87590ee3c9",
   "metadata": {},
   "source": [
    "#### 4.1. Trends and Seasonality: How does the timeseries look like at different scales?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea681fe3-41e7-42f5-a652-3d7300eb8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = [(None, None, \"(Complete)\"), (200, 300, \"(Index 200-300)\"), (300, 400, \"(Index 300-400)\")]\n",
    "for k in variants:\n",
    "    Helper.plot_time_series(\n",
    "        df, x_col=\"datetime\", y_col=\"temperature\", hue_col=\"property_name\",\n",
    "        title=f\"Temperature over Time {k[2]}\",xlabel=\"Time\", ylabel=\"Temperature [C°]\",\n",
    "        start=k[0], end=k[1], scatter=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba604cae-c2b7-4812-a716-834767f7b37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(x=df.datetime.dt.hour, y=df.temperature, hue=df.property_name)\n",
    "plt.title(\"Measurements during each hour of the day\")\n",
    "plt.xlabel(\"Hour of the day\")\n",
    "plt.ylabel(\"Temperature [C°]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379bca8f-3774-4fa4-a10a-d7554b6287af",
   "metadata": {},
   "source": [
    "- <b>Insight 1:</b> The timeseries data shows a non-equidistant distribution, likely caused by irregular sampling or missing values. Additionally, there is no trend visible; however, there may be a seasonal pattern that cannot be quantified through visual inspection alone.\n",
    "\n",
    "- <b>Insight 2:</b> Some gaps are that broad such that later interpolating these values might be not feasible, given the existing information (.e.g., ```2019-08``` to ```2019-10```).\n",
    "\n",
    "- <b>Insight 3:</b> The cooling system does not provide any information for the period from 0-6 am. The heating system seems to maintain a fairly constant temperature per hour, which indicates a periodicity of 24 hours.\n",
    "\n",
    "It would be convenient to use classical FFT at this point to find out more about the periodicity of the signal. However, this is problematic due to the non-equidistant sampling of the data. Therefore, we use Lomb-Scargle Periodograms to investigate periodicity, i.e., finding seasonal behavior in unevenely spaced data [1,2].\n",
    "\n",
    "[1] _Lomb, N. R. (1976). Least-squares frequency analysis of unequally spaced data. Astrophysics and space science, 39, 447-462._<br>\n",
    "[2] _Scargle, J. D. (1982). Studies in astronomical time series analysis. II-Statistical aspects of spectral analysis of unevenly spaced data. Astrophysical Journal, Part 1, vol. 263, Dec. 15, 1982, p. 835-853., 263, 835-853._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91d7c4-7b76-42aa-92cf-13fb029d8495",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [PROP_HEATING, PROP_COOLING]:\n",
    "    temp_series = df[df.property_name.isin([k])].copy()\n",
    "    \n",
    "    # converting datetime into seconds passed from first sample [t] and extracting temperature [y]\n",
    "    temp_series.datetime = temp_series.datetime - np.min(temp_series.datetime)\n",
    "    t = temp_series.datetime.dt.seconds + temp_series.datetime.dt.days*24*60*60\n",
    "    y = temp_series.temperature\n",
    "\n",
    "    # applying the Lomb-Scargle Periodograms and plotting the result\n",
    "    frequency, power = LombScargle(t, y).autopower()\n",
    "    frequencies = pd.DataFrame([frequency, power]).transpose()\n",
    "    frequencies.columns = ['frequency', 'power']\n",
    "    Helper.plot_time_series(\n",
    "        frequencies, x_col=\"frequency\", y_col=\"power\",\n",
    "        title=f'Lomb-Scargle Periodogram ({k})', xlabel=\"Frequency (Hz)\", ylabel=\"Power\",\n",
    "    )\n",
    "\n",
    "    # find the ten most \"powerful\" frequencies\n",
    "    indices = np.argpartition(-power, 10)[:10]\n",
    "    indices = indices[np.argsort(-power[indices])]\n",
    "    max_frequency = frequency[indices]\n",
    "    \n",
    "    # convert frequencies from Hz back to hours and print out the detected periodicities\n",
    "    period = np.round((1 / max_frequency) / 3600, 1) \n",
    "    print(\"The detected frequencies (hour, power) in descending order:\", Helper.print_distinct_values(period, power[indices], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b525a-0466-4e37-956c-3d88e3831ea2",
   "metadata": {},
   "source": [
    "- <b>Insights 4:</b> There are signs of seasonality in the data, particularly for the heating time series. It shows a recurring pattern after 24 hours (power: 0.2), 12 hours (power: 0.13) and little after 8 hours (power: 0.05). For the cooling series there is a relatively weak recurring pattern in 24 hour periods (power: 0.09)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b579a04-7285-4577-9089-fd9659676af8",
   "metadata": {},
   "source": [
    "#### 4.2. Sampling rate: What would be a good frequency?\n",
    "\n",
    "To obtain an equidistant sampling rate, which is required for different machine learning approaches, an appropriate sampling rate must be determined. Though the objective is to obtain a fine-grained time series, the sampling rate should not be so high that the generation of the missing values becomes impractical. Based on the previous analysis, an hourly sampling rate seems to be suitable. To confirm this, we want to examine the distribution of time gaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c09fb29-54e5-439c-af0f-f99dc411e6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two columns that present the difference in temperature/time to the preceding sample, grouped by heating and cooling\n",
    "df[['time_diff', 'temp_diff']] = df.groupby('property_name', observed=False)[['datetime', 'temperature']].diff()\n",
    "\n",
    "# It is more convenient to transform the datetime difference into seconds, i.e., a numeric value, for further processing.\n",
    "df['time_diff'] = df.time_diff.dt.days * 24 * 3600.0 + df.time_diff.dt.seconds\n",
    "\n",
    "# The first two rows are nan, as they was no preceding sample -> filling with the median (robust in regards to outier)\n",
    "df[[\"time_diff\", \"temp_diff\"]] = df[[\"time_diff\", \"temp_diff\"]].apply(lambda x: x.fillna(x.median()), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ee81a-d766-41fb-9fc7-080c050fd10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_SIX_HOURS = 3600*6\n",
    "BINS = 100\n",
    "\n",
    "# Filtering to samples with timestep of less than six hours (sampling rate is liekly smaller) to not skew the histogram\n",
    "g = sns.FacetGrid(df[df.time_diff < SECONDS_SIX_HOURS], col=\"property_name\", height=5, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot, x=\"time_diff\", bins=BINS, stat=\"frequency\")\n",
    "g.set_axis_labels(\"Time Difference [sec]\", \"Density\")\n",
    "g.set_titles(col_template=\"Property: {col_name}\", size=15)\n",
    "g.set_xlabels(size=12)\n",
    "g.set_ylabels(size=12)\n",
    "plt.show()\n",
    "\n",
    "# Supporting the visual histplot by creating a histogram and printing the most occuring value as numeric value\n",
    "for k in [PROP_HEATING, PROP_COOLING]:\n",
    "    temp = df[(df.property_name.isin([k])) & (df.time_diff < SECONDS_SIX_HOURS)].copy()\n",
    "    histogram = np.histogram(temp.time_diff[1:], BINS)\n",
    "    i_max = np.argmax(histogram[0])\n",
    "    print(f\"Most occuring (n={histogram[0][i_max]}) time difference for {k}:\", histogram[1][i_max])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab55067-640a-48d3-bf83-6bf3bd8c92f5",
   "metadata": {},
   "source": [
    "- <b>Insight 5:</b> The majority of sampled time steps have a distance of ~one hour and makes it a suitable candidate for sampling rate when turning the timeseries evenly spaced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b13c4-4d6d-421b-a871-0e506ef6cc66",
   "metadata": {},
   "source": [
    "#### 4.3. Transition Probabilities: How are the temperature and the n-th order difference distributed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6fb3b-e059-415f-8e8c-c20072a96199",
   "metadata": {},
   "source": [
    "We plot histograms to understand the distribution of temperature and the differences between consecutive measurements for both the cooling and heating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9fa1da-5162-4bc9-991e-2beb339094fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"property_name\", height=5, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot, x=\"temperature\", bins=20, stat='density')\n",
    "g.set_axis_labels(\"Temperature [C°]\", \"Density\")\n",
    "g.set_titles(col_template=\"Distribution of the Temperature: {col_name}\", size=15)\n",
    "g.set_xlabels(size=12)\n",
    "g.set_ylabels(size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd2615-f063-4959-982b-113e9f6dc885",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"property_name\", height=5, aspect=1.5)\n",
    "g.map_dataframe(sns.histplot, x=\"temp_diff\", bins=100, stat='density')\n",
    "g.set_axis_labels(\"Temperature Difference [C°]\", \"Density\")\n",
    "g.set_titles(col_template=\"Distribution of Consecutive Temperatures: {col_name}\", size=15)\n",
    "g.set_xlabels(size=12)\n",
    "g.set_ylabels(size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c38f0e8-ee66-455a-b1b6-b1467c075a50",
   "metadata": {},
   "source": [
    "- <b>Insight 6:</b> All histograms suggest a Gaussian pattern in the distribution of temperature and adjacent values. Specifically, the temperatures of the heating system appear to follow a normal distribution, while the cooling system shows a Gaussian mixture distribution with three peaks in the top graph and five peaks in the bottom graph. Formally, let $X_{t}$ represent the heating temperature at time $t$ and $Y_{t}$ represent the cooling temperature at time $t$. Thus, we infer:\n",
    "  - $P(X_{t}) \\sim \\mathcal{N}(\\mu, \\sigma)$\n",
    "  - $P(X_{t-1} - X_{t}) \\sim \\mathcal{N}(\\mu, \\sigma)$\n",
    "  - $P(Y_{t}) \\sim \\sum_{i=1}^{3} \\pi_{i} \\mathcal{N}(\\mu_{i}, \\sigma_{i})$\n",
    "  - $P(Y_{t-1} - Y_{t}) \\sim \\sum_{i=1}^{5} \\pi_{i} \\mathcal{N}(\\mu_{i}, \\sigma_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737bc06-b9ec-4c7e-9f04-67f58b2b2dbf",
   "metadata": {},
   "source": [
    "While most samples align with the targeted hourly sampling frequency, caution is needed when inferring a distribution for the differences between consecutive values. The timesteps are not equidistant and may not accurately represent the true hourly interval distribution. To address this, we extend the histogram analysis by incorporating a time-axis, evaluating not only with a ```lag=1``` but also up to the previous 50 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c13252-4cd3-478f-925b-5c2e2210b24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperatue_lags = pd.DataFrame()\n",
    "lags = range(1, 50)\n",
    "\n",
    "# calculate the temperature difference for 1...50 lags\n",
    "for k in lags:\n",
    "    temp = df.groupby('property_name', observed=False)[['datetime', 'temperature']].diff(k)\n",
    "    temp['property_name'] = df.property_name\n",
    "    temperatue_lags = pd.concat([temperatue_lags, temp])\n",
    "\n",
    "# with calculating lags for 1...50, nan values arise, which we cannot use for this analysis\n",
    "temperatue_lags.dropna(inplace=True)\n",
    "temperatue_lags.reset_index(drop=True, inplace=True)\n",
    "temperatue_lags.datetime = temperatue_lags.datetime.dt.days*24 + temperatue_lags.datetime.dt.seconds / 3600.0\n",
    "temperatue_lags.datetime = temperatue_lags.datetime.astype(int)\n",
    "\n",
    "g = sns.FacetGrid(temperatue_lags, col=\"property_name\", height=5, aspect=1)\n",
    "g.map_dataframe(sns.kdeplot, x=\"datetime\", y=\"temperature\", fill=True)\n",
    "g.set_axis_labels(\"Diff. Hours\", \"Diff. Temperature\")\n",
    "g.set_titles(col_template=\"Property: {col_name}\", size=15)\n",
    "g.set_xlabels(size=12)\n",
    "g.set_ylabels(size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a95560-46e9-4502-9448-41bd72f22681",
   "metadata": {},
   "source": [
    "- <b>Insight 7:</b> Both Gaussian mixture like distribution are also observable when an additional axis represents the time gap between two measurements.\n",
    "\n",
    "\n",
    "- <b>Simplification:</b> We assume a normal distribution as a prior for the heating values (```temperature``` and ```temp_diff```) and a Gaussian mixture model for the cooling values. This assumption will be particularly useful later when filling in missing data in the time series using transition probabilities. A more complex approach would involve modeling the joint distribution of temperature changes over a given time period and using the marginal distribution of temperature when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a980a1ac-1194-410e-96ae-fc29b3a75af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a Gaussian Mixture distribution for the different scenarios\n",
    "# Note: When n_components=1 the Gaussian Mixture distribution \"collapses\" to a Gaussian distribution\n",
    "gm_heating_d = GaussianMixture(n_components=1).fit(df[df.property_name == PROP_HEATING].temperature.values.reshape(-1,1))\n",
    "gm_cooling_d = GaussianMixture(n_components=5).fit(df[df.property_name == PROP_COOLING].temperature.values.reshape(-1,1))\n",
    "gm_heating_t = GaussianMixture(n_components=1).fit(df[df.property_name == PROP_HEATING].temperature.values.reshape(-1,1))\n",
    "gm_cooling_t = GaussianMixture(n_components=3).fit(df[df.property_name == PROP_COOLING].temperature.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8bfc18-b0ba-4a69-9e24-9053ed3b66d1",
   "metadata": {},
   "source": [
    "#### 4.4. Interaction: What is the relation of heating and cooling temperature?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f6a5-6141-481e-b6a8-7a6958836e26",
   "metadata": {},
   "source": [
    "Not only the series itself can show anomalies, but also the interaction between the two series. We are therefore interested in whether we can derive any patterns from the gap between warming and cooling, which is also a time series. Therefore, we follow the same approach as in previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52350043-9bfc-4afd-b6de-fe1ef1b290a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify samples for which we have cooling and heating information at the same time\n",
    "index_duplicates = df[df.datetime.duplicated()].datetime\n",
    "df_res = Helper.calculate_difference_timeseries(df[df.datetime.isin(index_duplicates)])\n",
    "df_res.columns = [\"datetime\", \"residuals\", \"heating\", \"cooling\"]\n",
    "df_res.datetime = pd.to_datetime(df_res.datetime)\n",
    "df_res[\"res_diff\"] = df_res.residuals.diff()\n",
    "df_res.res_diff =  df_res.res_diff.fillna(df_res.res_diff.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dd633d-2145-47cb-a846-f9ceba551452",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_res.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e56591-2d75-4a79-b783-604ada073075",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_res.residuals, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46034c34-2c7e-4d1b-a7b6-882e5567fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_list = [df_res[['datetime', 'residuals']].diff(k) for k in range(1, 50)]\n",
    "residual_lags = pd.concat(res_list, ignore_index=True)\n",
    "residual_lags['datetime'] = residual_lags['datetime'].dt.days * 24 + residual_lags['datetime'].dt.seconds / 3600.0\n",
    "residual_lags.dropna(inplace=True)\n",
    "residual_lags['datetime'] = residual_lags['datetime'].astype(int)\n",
    "\n",
    "g = sns.kdeplot(residual_lags, x=\"datetime\", y=\"residuals\", fill=True)\n",
    "g.set_title(\"Transition probability for the residuals of heating and cooling\")\n",
    "g.set_xlabel(\"Diff. Hours\", size=12)\n",
    "g.set_ylabel(\"Diff. Residuals\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecbd28d-e804-444a-9128-147aa7f2721f",
   "metadata": {},
   "source": [
    "- <b>Insight 8:</b> We visually estimate a Gaussian mixture model (three peaks) for both distributions (it poses a simplifcation as well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b5c569-beaa-450d-950e-4d4a927dabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# likewise we fit a gaussian mixture distribution for the residual values\n",
    "gm_residuals_t = GaussianMixture(n_components=3).fit(df_res.residuals.values.reshape(-1,1))\n",
    "gm_residuals_d = GaussianMixture(n_components=3).fit(df_res.res_diff.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063fcd8d-27c2-40e5-9b76-ab4776080204",
   "metadata": {},
   "source": [
    "### 5. Modelling\n",
    "\n",
    "From the explorative data analysis, we infer the following task specification:\n",
    "- <b>Task 1:</b> Address the missing values in the time series, considering the known seasonality patterns and fitted distributions.\n",
    "- <b>Task 2:</b> Identify anomalies in the temperature system, whether they occur in the heating system, cooling system, or both."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6835789-0219-4acd-952a-435ef6e769c3",
   "metadata": {},
   "source": [
    "#### 5.1. How about a first baseline anomaly detection model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3142678-b86d-44c7-a122-39123f82a7b0",
   "metadata": {},
   "source": [
    "As a baseline anomaly detection model, we can use the fitted distributions and look for associated values with a low occurence probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae8bab4-91fd-46a1-aa08-ee6c4def1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a constant [in %] for the quantile, which sets the static threshold for detecting anomalies\n",
    "QUANTILE_OUTLIER = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb64c76-c521-49cc-8732-dc7b098cd397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our six fitted distributions, we can identify six anomalies\n",
    "anomaly_columns = [\n",
    "    \"outlier_heating_d\", \"outlier_cooling_d\", \"outlier_residuals_d\",\n",
    "    \"outlier_heating_t\", \"outlier_cooling_t\", \"outlier_residuals_t\",\n",
    "]\n",
    "df[anomaly_columns] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2168864-3cf3-49be-8267-cf2b2807a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate for each sample the probability of occurence\n",
    "proba_heating_t = np.exp(gm_heating_t.score_samples(df[df.property_name == PROP_HEATING].temperature.values.reshape(-1,1)))\n",
    "proba_cooling_t = np.exp(gm_cooling_t.score_samples(df[df.property_name == PROP_COOLING].temperature.values.reshape(-1,1)))\n",
    "proba_residuals_t = np.exp(gm_residuals_t.score_samples(df_res.residuals.values.reshape(-1,1)))\n",
    "\n",
    "proba_heating_d = np.exp(gm_heating_d.score_samples(df[df.property_name == PROP_HEATING].temp_diff.values.reshape(-1,1)))\n",
    "proba_cooling_d = np.exp(gm_cooling_d.score_samples(df[df.property_name == PROP_COOLING].temp_diff.values.reshape(-1,1)))\n",
    "proba_residuals_d = np.exp(gm_residuals_d.score_samples(df_res.residuals.values.reshape(-1,1)))\n",
    "\n",
    "# Define the threshold for anomalies based on the quantiles of the probabilities of occurrence\n",
    "quantile_heating_t = np.percentile(proba_heating_t, QUANTILE_OUTLIER)\n",
    "quantile_cooling_t = np.percentile(proba_cooling_t, QUANTILE_OUTLIER)\n",
    "quantile_residuals_t = np.percentile(proba_residuals_t, QUANTILE_OUTLIER)\n",
    "\n",
    "quantile_heating_d = np.percentile(proba_heating_d, QUANTILE_OUTLIER)\n",
    "quantile_cooling_d = np.percentile(proba_heating_d, QUANTILE_OUTLIER)\n",
    "quantile_residuals_d = np.percentile(proba_heating_d, QUANTILE_OUTLIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97008f94-b99d-4a5f-8ed0-c69914a03616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the samples as anomalies based on the threshold\n",
    "# Note: Duplicate the residual probabilities since they appear for both cooling and heating\n",
    "df.loc[df.property_name == PROP_HEATING, \"outlier_heating_t\"] = proba_heating_t < quantile_heating_t\n",
    "df.loc[df.property_name == PROP_COOLING, \"outlier_cooling_t\"] = proba_cooling_t < quantile_cooling_t\n",
    "df.loc[df.datetime.isin(index_duplicates), \"outlier_residual_t\"] = np.repeat(proba_residuals_t, 2) < quantile_residuals_t\n",
    "\n",
    "df.loc[df.property_name == PROP_HEATING, \"outlier_heating_d\"] = proba_heating_d < quantile_heating_d\n",
    "df.loc[df.property_name == PROP_COOLING, \"outlier_cooling_d\"] = proba_cooling_d < quantile_cooling_d\n",
    "df.loc[df.datetime.isin(index_duplicates), \"outlier_residual_d\"] = np.repeat(proba_residuals_d, 2) < quantile_residuals_d\n",
    "\n",
    "\n",
    "df[\"outlier_score\"] = df.outlier_heating_t.astype(int) + df.outlier_cooling_t.astype(int) + df.outlier_residuals_t.astype(int) +  df.outlier_heating_d.astype(int) + df.outlier_cooling_d.astype(int) + df.outlier_residuals_d.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f96743-4872-44a9-8ca8-46586c61d04a",
   "metadata": {},
   "source": [
    "Let's have a look at the detected anomalies from our first attempt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f065ce6-9e61-480d-b34b-ab252b2ce8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom = df.iloc[250:300,:]\n",
    "plt.figure(figsize=(15,5))\n",
    "sns.lineplot(zoom, x='datetime', y='temperature', hue=\"property_name\")\n",
    "for k in anomaly_columns:\n",
    "    sns.scatterplot(zoom[(zoom[k] == True) & (zoom[\"outlier_score\"] > 1)], x='datetime', y='temperature', color='red', s=70, marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5196c-cda7-482a-9e81-7a6d354b3040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total number of outliers:\", len(df[df[\"outlier_score\"] > 1].datetime.unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a4f209-4506-4f1b-84ed-ee5455c8cba7",
   "metadata": {},
   "source": [
    "#### 5.2. Missing Values and Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d4f6e-5a82-49f3-a809-f186dde7a4c2",
   "metadata": {},
   "source": [
    "There are various methods for interpolating missing values, such as forward/backward fill, linear interpolation, and spline interpolation. However, the challenge with the current time series is the large gaps between measurements. Fortunately, we have some knowledge of the generating factors. The primary goal is not to reconstruct the time series exactly (which is likely not feasible), but to capture the generative factors such as seasonality and the transition probabilities. This approach aims to ensure that the subsequent training of a machine learning model for outlier detection is based on iid data. However, for really large gaps, error propagation and the associated uncertainty quantification might be so high that interpolation may not be feasible. Therefore, we add a marker so that, on demand, the respective sequences can be excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0312db0-bd08-49df-9b93-6f5730b09716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce a constant that marks breaks in timeseries\n",
    "MAX_TIMEGAP_SECONDS = 3600*24 # one day\n",
    "larger_defined_timegap = df['time_diff'] > MAX_TIMEGAP_SECONDS\n",
    "# Additionally, we need to set the breakpoint at the preceding sample to indicate the start of the gap\n",
    "index_heating = df.property_name == PROP_HEATING\n",
    "index_cooling = df.property_name == PROP_COOLING\n",
    "for k in [PROP_HEATING, PROP_COOLING]:\n",
    "    index = df.property_name == k\n",
    "    df.loc[index, 'break'] = larger_defined_timegap[index] | larger_defined_timegap[index].shift(-1, fill_value=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe9b26-5f30-4de1-b768-5ed58e03effb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's aggregate the data into one-hour intervals using the mean for temperatures.\n",
    "# Hours with no available values will be assigned NaN.\n",
    "df.set_index(\"datetime\", inplace=True)\n",
    "df = pd.DataFrame(df.groupby(by=[\"property_name\", pd.Grouper(freq='1h')], observed=False).agg({\n",
    "    \"temperature\": 'mean',\n",
    "    \"break\": 'max',\n",
    "})).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbc302f-e060-4939-9ea8-e3fdea228085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a heating and cooling series\n",
    "# Note: The timeseries for cooling and heating have identical start and end timestamps\n",
    "# No need to artificially extend either series to align their start and end times.\n",
    "series_heating = df[df.property_name == PROP_HEATING][['datetime', 'temperature', 'break']]\n",
    "series_cooling = df[df.property_name == PROP_COOLING][['datetime', 'temperature', 'break']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbba0d70-5102-4663-8f32-e1c153e5010d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fill the NaN values in the series using seasonality information.\n",
    "# The function iteratively fills NaNs when a value is available for the same time in the past, based on the specified seasonality.\n",
    "# The recursive process stops when the series is no longer updating.\n",
    "# To prevent repeatedly propagating an outlier, the function checks for outliers using the provided distribution.\n",
    "# Note: For simplicity, we are only using the temperature distribution, not the distribution of consecutive values\n",
    "series_heating.temperature = Helper.fill_nan_from_seasonality(series_heating.temperature, 24, gm_heating_t, quantile_heating_t)\n",
    "series_heating.temperature = Helper.fill_nan_from_seasonality(series_heating.temperature, 12, gm_heating_t, quantile_heating_t)\n",
    "series_cooling.temperature = Helper.fill_nan_from_seasonality(series_cooling.temperature, 24, gm_cooling_t, quantile_cooling_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f5300a-9d4f-4b06-95f0-b1ef6de23625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the remaining missing values by sampling from the temperature distributions\n",
    "# Note: Also here we'd generate better results when including the distribution on consectuive values as well\n",
    "series_heating.temperature = Helper.fill_nan_from_distribution(series_heating.temperature, gm_heating_t).copy()\n",
    "series_cooling.temperature = Helper.fill_nan_from_distribution(series_cooling.temperature, gm_cooling_t).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c502975d-60f4-40ff-baf8-44c28694873d",
   "metadata": {},
   "source": [
    "Let's get an idea of the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad9d55-609f-47a1-864e-29348f0ab133",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(series_heating.datetime, series_heating.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6034cde-c3b7-45ba-bd49-05995bc99075",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.plot(series_cooling.datetime, series_cooling.temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc9e8a-9e38-4084-915d-fb092a41425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is important for further processing that both series do represent the same dates\n",
    "assert len(series_heating) == len(series_cooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b5374-12d1-4b56-964c-951d14e6c080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to upsampling, the break column contains NaNs.\n",
    "# We replace NaNs with True if they are located between True values, and with False otherwise.\n",
    "series_heating.loc[:,\"break\"] = series_heating.loc[:,\"break\"].infer_objects(copy=False)\n",
    "series_cooling.loc[:,\"break\"] = series_cooling.loc[:,\"break\"].infer_objects(copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7facb69e-76cd-4863-a0d9-cbfe71bb3bd2",
   "metadata": {},
   "source": [
    "For the cooling temperatures, we set the time period from 0:00 to 6:00 AM to True. Although we are currently using the sampled values from this range for simplicity, we may choose to exclude them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f69e88-9190-4bcd-9c9b-bf40bafc0ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_night_time = (series_cooling.datetime.dt.hour >= 0) & (series_cooling.datetime.dt.hour <= 6)\n",
    "series_cooling.loc[index_night_time, \"break\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a4e1d-8a1b-493e-996c-6991d51177f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_heating.reset_index(inplace=True, drop=True)\n",
    "series_cooling.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af27f96c-963a-4031-be66-ea8c0d21fbcd",
   "metadata": {},
   "source": [
    "#### 5.3. Feature Engineering and Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da59e389-f21a-43db-9950-a9de3e7672ee",
   "metadata": {},
   "source": [
    "When splitting data into training, validation, and test sets, it's important to avoid introducing bias. For instance, in the case of a time series, you should partition longer continuous periods for testing rather than selecting individual values at random. If you cut out singular values, the model might simply interpolate and predict the missing values, leading to artificially high performance metrics. Properly segmenting continuous blocks helps ensure the model is genuinely evaluated on unseen data, providing a more accurate measure of its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a3dad7-3dc9-4636-a1a6-eb2382f7a051",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = series_heating.merge(series_cooling, how=\"left\", on=\"datetime\")\n",
    "series.columns = [\"datetime\", \"temperature_heating\", \"break_heating\", \"temperature_cooling\", \"break_cooling\"]\n",
    "series[\"residuals\"] = series.temperature_heating - series.temperature_cooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71d4953-34de-48b6-ae34-dc530538da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity we are splitting the data linearly by consecutive segments\n",
    "# However, there is a sound class from sklearn - TimeSeriesSplit - which would be a more professional alternative\n",
    "n = len(series)\n",
    "n_train = int(0.8*n)\n",
    "n_val = int(0.1*n)\n",
    "n_test = n - n_train - n_val\n",
    "\n",
    "assert (n_train + n_val + n_test) == n \n",
    "\n",
    "train_df= series.loc[0:n_train, [\"temperature_heating\", \"temperature_cooling\", \"residuals\"]]\n",
    "val_df= series.loc[n_train:(n_train+n_val), [\"temperature_heating\", \"temperature_cooling\", \"residuals\"]]\n",
    "test_df= series.loc[(n_train+n_val):, [\"temperature_heating\", \"temperature_cooling\", \"residuals\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c806cd-681e-4a5d-b373-44aed45b04cc",
   "metadata": {},
   "source": [
    "Furthermore, we want the data to be standardized, as it holds advantages regarding the stability of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b3561-267f-4d68-86ca-f9e463a65d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = train_df.mean()\n",
    "train_std = train_df.std()\n",
    "\n",
    "train_df = (train_df - train_mean) / train_std\n",
    "val_df = (val_df - train_mean) / train_std\n",
    "test_df = (test_df - train_mean) / train_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abb709a-0bc8-4e3d-86e3-7bf4f0acae86",
   "metadata": {},
   "source": [
    "The upcoming strategy utilizes a windowing approach that segments the data into windows of length 25 with a step length of 1 (as illustrated in the image below). We aim to use 24 hours of data as the input to predict the value of the 25th hour. This method and its implementation were originally presented by Google, and I would like to acknowledge their contribution: [Time Series Forecasting](https://www.tensorflow.org/tutorials/structured_data/time_series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa865d5b-a4b9-4dc0-97f1-3fa7f2c42508",
   "metadata": {},
   "source": [
    "![Window Image](https://raw.githubusercontent.com/tensorflow/docs/master/site/en/tutorials/structured_data/images/raw_window_1h.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c048c0-9a7c-4c3b-b535-caadc75bb45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERIES_LENGTH=25\n",
    "window_series = WindowGenerator(\n",
    "    input_width=SERIES_LENGTH, label_width=1, shift=1,\n",
    "    train_df=train_df, val_df=val_df, test_df=test_df,\n",
    "    label_columns=['temperature_heating'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80747e26-1047-49be-962d-ec8b935fc9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_window = tf.stack([np.array(train_df[:window_series.total_window_size]),\n",
    "                           np.array(train_df[100:100+window_series.total_window_size]),\n",
    "                           np.array(train_df[300:300+window_series.total_window_size])])\n",
    "\n",
    "example_inputs, example_labels = window_series.split_window(example_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0ed7a-47bf-4f7c-a909-c5ce44477b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_series.plot(plot_col='temperature_heating')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cfca5f-122e-4901-8a62-df77c9e5b5d9",
   "metadata": {},
   "source": [
    "#### 5.4. Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0396b1e0-80f2-4b52-a491-a4141b8bd81e",
   "metadata": {},
   "source": [
    "Our approach is to train a model that can predict the next value of our time series. We assume that values with a large residual between the predicted and actual values indicate outliers.\n",
    "\n",
    "- <b>Simplification:</b> We limit our approach to the heating time series. While it still utilizes cooling temperature and residual data for training and predicting, it does not actively predict anomalies for them. However, extending the model training to these two time series can be done similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d5187b-add0-4a99-b236-e70a1ff38974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collection of (yet simple) models that allow for an easy extension\n",
    "model_zoo = {\n",
    "    'linear': tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "        tf.keras.layers.Reshape((1, 1)),\n",
    "    ]),\n",
    "    'convolution': tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=32, kernel_size=(24,), activation='relu'),\n",
    "        tf.keras.layers.Dense(units=32, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "        tf.keras.layers.Reshape([1, -1])\n",
    "    ]),\n",
    "    'lstm': tf.keras.models.Sequential([\n",
    "        tf.keras.layers.LSTM(32, return_sequences=True),\n",
    "        tf.keras.layers.Dense(units=1),\n",
    "        tf.keras.layers.Reshape([1, -1]),\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0e9df6-e6fb-40c9-b420-b35c16e02f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard code for compiling and fitting a tensorflow/keras model\n",
    "def compile_and_fit(model, window, patience=2):\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience, mode='min')\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.MeanSquaredError(),\n",
    "        optimizer=tf.keras.optimizers.Adam(),\n",
    "        metrics=[tf.keras.metrics.MeanAbsoluteError()],\n",
    "    )\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS, validation_data=window.val, callbacks=[early_stopping], verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68089076-26b7-4053-89d6-a0a645012cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPOCHS = 20\n",
    "val_performance = {}\n",
    "performance = {}\n",
    "models = {}\n",
    "\n",
    "# Train all the models and store the results (performance and the model itself)\n",
    "for k in model_zoo:\n",
    "    print(f\"Training and evaluating {k} model\")\n",
    "    model = model_zoo[k]\n",
    "    history = compile_and_fit(model, window_series)\n",
    "    val_performance[k] = model.evaluate(window_series.val, return_dict=True)\n",
    "    performance[k] = model.evaluate(window_series.test, verbose=0, return_dict=True)\n",
    "    models[k] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324366f-3c9f-4789-842b-0232b7033c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplary plotting the results for the linear model\n",
    "window_series.plot(models['linear'], plot_col=\"temperature_heating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534f12bc-6e53-495c-9452-72f556b9f7c5",
   "metadata": {},
   "source": [
    "### 6. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099aa172-1228-48e2-b30f-679c11dbbbe3",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of the models on both the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3075f-6185-4298-b717-007315084c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval_val = pd.DataFrame(val_performance).transpose()\n",
    "df_eval_val[\"Dataset\"] = \"Validation\"\n",
    "\n",
    "df_eval_test = pd.DataFrame(performance).transpose()\n",
    "df_eval_test[\"Dataset\"] = \"test\" \n",
    "df_val = pd.concat([df_eval_val, df_eval_test]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a445a-187c-4618-92ca-e3fed71ad3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "g = sns.barplot(df_val,x=\"index\", y=\"mean_absolute_error\", hue=\"Dataset\")\n",
    "g.set_title(\"Validation and Test errors\")\n",
    "g.set_xlabel(\"Model\", size=12)\n",
    "g.set_ylabel(\"Mean Absolute error\", size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dd247e-c2a2-49e2-93c8-0f313a91f21b",
   "metadata": {},
   "source": [
    "Amond the three models, the perceptron (linear model) performed best. Therefore, we will use the linear model to illustrate a potential evaluation for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f19d27-c30e-4768-81af-9f27aa9a90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the train data set and calculate the difference towards the actual value\n",
    "result = models['linear'].predict(window_series.train)\n",
    "difference = result[:, 0, 0] - train_df[SERIES_LENGTH:].temperature_heating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c66c26-343e-4317-9aa9-981a5e420378",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.histplot(difference)\n",
    "g.set_title(\"Histogram: Absolute error of predicted and actual value\")\n",
    "g.set_xlabel(\"Temperature (Heating)\", size=12)\n",
    "g.set_ylabel(\"Count\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1776088d-8ece-4831-98b7-7fdeb58ebc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the outlier thresholds for both sides of the distribution based on the difference values\n",
    "bounds = np.quantile(difference, [QUANTILE_OUTLIER/2.0/100.0, 1-QUANTILE_OUTLIER/2.0/100.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2751aeae-f232-4bd6-bcf9-72bb7cafe0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference of prediction and actual value\n",
    "train_df[\"difference\"] = models['linear'].predict(window_series.train)[:, 0, 0] - train_df[SERIES_LENGTH:].temperature_heating\n",
    "val_df[\"difference\"] = models['linear'].predict(window_series.val)[:, 0, 0] - val_df[SERIES_LENGTH:].temperature_heating\n",
    "test_df[\"difference\"] = models['linear'].predict(window_series.test)[:, 0, 0] - test_df[SERIES_LENGTH:].temperature_heating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ba71d-a9cd-49d4-9035-21f817b9131d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether the diference values exceed the bounds\n",
    "train_df[\"outlier\"] = train_df['difference'].apply(lambda x: not bounds[0] <= x <= bounds[1])\n",
    "val_df[\"outlier\"] = val_df['difference'].apply(lambda x: not bounds[0] <= x <= bounds[1])\n",
    "test_df[\"outlier\"] = test_df['difference'].apply(lambda x: not bounds[0] <= x <= bounds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93497e06-c4d7-444b-8f67-12ccf7f62bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"datetime\"] = series.loc[0:n_train, \"datetime\"]\n",
    "val_df[\"datetime\"] = series.loc[n_train:(n_train + n_val), \"datetime\"]\n",
    "test_df[\"datetime\"] = series.loc[(n_train + n_val):, \"datetime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147679d7-08e4-448b-92d2-c25894f7b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([train_df, val_df, test_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6f820-c089-4661-a847-07da3449d875",
   "metadata": {},
   "outputs": [],
   "source": [
    "zoom = df_final.iloc[250:300,:]\n",
    "plt.figure(figsize=(15,5))\n",
    "g = sns.lineplot(zoom, x='datetime', y='temperature_heating')\n",
    "sns.scatterplot(zoom[(zoom[\"outlier\"] == True)], x='datetime', y='temperature_heating', color='red', s=70, marker='o')\n",
    "g.set_title(\"Heating series with highlighted outliers\")\n",
    "g.set_xlabel(\"Date\", size=12)\n",
    "g.set_ylabel(\"Temperature (standardized)\", size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0948f877-c411-4074-9775-badab56763b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_outliers = len(df_final[df_final.outlier == True])\n",
    "ratio = np.round(n / n_outliers, 2)\n",
    "print(f\"In total we detected {n_outliers} potential anomalies for the heating temperature. Ratio: {ratio} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947e341-d72f-4402-be73-ad05dbb05eb1",
   "metadata": {},
   "source": [
    "Certainly, an anomaly rate of over five percent is quite high. The next step could be to refine all parts of the process, including better sampling for missing values, refining the modeling process, and exploring further machine learning approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6e1f5d-e2f4-4591-ab1c-a576b0d9c0b2",
   "metadata": {},
   "source": [
    "<b>Thank you for reading till the end 🙂</b>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
